@book{Box2015,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}


@misc{cdc2016,
author = {{Centers for Disease Control and Prevention}},
title = {{Overview of Influenza Surveillance in the United States}},
url = {https://www.cdc.gov/flu/weekly/overview.htm},
urldate = {February 8, 2017},
year = {2016}
}

@misc{cdc2016-baselines,
author = {{Centers for Disease Control and Prevention}},
title = {{Regional baseline values for influenza-like illness}},
url = {https://github.com/cdcepi/FluSight-forecasts/blob/master/wILI{\_}Baseline.csv},
urldate = {2017-02-07},
year = {2016}
}

@misc{cdc-decisions-2016,
author = {{Centers for Disease Control and Prevention}},
title = {{Staying Ahead of the Curve: Modeling and Public Health Decision-Making}},
url = {http://www.cdc.gov/cdcgrandrounds/archives/2016/january2016.htm},
urldate = {January 19, 2016},
year = {2016}
}

@article{Chakraborty2014,
abstract = {Abstract Modern epidemiological forecasts of common illnesses, such as the flu, rely on both traditional surveillance sources as well as digital surveillance data. However, most published studies have been retrospective. Concurrently, the reports about flu activity ... 
},
address = {Philadelphia, PA},
author = {Chakraborty, Prithwish and Khadivi, Pejman and Lewis, Bryan and Mahendiran, Aravindan and Chen, Jiangzhuo and Butler, Patrick and Nsoesie, Elaine O and Mekaru, Sumiko R and Brownstein, John S and Marathe, Madhav V and Ramakrishnan, Naren},
file = {:Users/ngr/Documents/Mendeley Desktop/Chakraborty et al/Chakraborty et al. - 2014.pdf:pdf},
issn = {978-1-61197-344-0},
journal = {Proceedings of the 2014 SIAM International Conference on Data Mining },
pages = {262--270},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Forecasting a Moving Target: Ensemble Models for ILI Case Count Predictions}},
url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611973440.30 papers2://publication/doi/10.1137/1.9781611973440.30},
year = {2014}
}

@Manual{xgboost,
    title = {xgboost: Extreme Gradient Boosting},
    author = {Tianqi Chen and Tong He and Michael Benesty},
    year = {2016},
    note = {R package version 0.4-4},
    url = {https://CRAN.R-project.org/package=xgboost},
}

@article{Chretien2014,
abstract = {Abstract Forecasts of influenza activity in human populations could help guide key preparedness tasks. We conducted a scoping review to characterize these methodological approaches and identify research gaps. Adapting the PRISMA methodology for systematic ... 
},
author = {Chretien, Jean-Paul and George, Dylan and Shaman, Jeffrey and Chitale, Rohit A and McKenzie, F Ellis},
file = {:Users/ngr/Documents/Mendeley Desktop/Chretien et al/Chretien et al. - 2014.pdf:pdf},
journal = {PloS one},
number = {4},
pages = {e94130},
publisher = {Public Library of Science},
title = {{Influenza Forecasting in Human Populations: A Scoping Review}},
url = {http://dx.plos.org/10.1371/journal.pone.0094130.g002 papers2://publication/doi/10.1371/journal.pone.0094130},
volume = {9},
year = {2014}
}

@inproceedings{cortes2014ensembleStructuredPrediction,
  title={Ensemble Methods for Structured Prediction},
  author={Cortes, Corinna and Kuznetsov, Vitaly and Mohri, Mehryar},
  booktitle={Proceedings of The 31st International Conference on Machine Learning},
  pages={1134--1142},
  year={2014}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@inproceedings{ganti2011cake,
  title={Cake: Convex adaptive kernel density estimation},
  author={Ganti, Ravi and Gray, Alexander},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={498--506},
  year={2011}
}

@article{Gneiting2007,
author = {Gneiting, Tilmann and Raftery, Adrian E},
file = {:Users/ngr/Documents/Mendeley Desktop/Gneiting, Raftery/Gneiting, Raftery - 2007.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {477},
pages = {359--378},
publisher = {Taylor {\&} Francis Group },
title = {{Strictly proper scoring rules, prediction, and estimation}},
url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2{\&}SrcAuth=mekentosj{\&}SrcApp=Papers{\&}DestLinkType=FullRecord{\&}DestApp=WOS{\&}KeyUT=000244361000032 papers2://publication/doi/10.1198/016214506000001437},
volume = {102},
year = {2007}
}

@book{Hastie2011,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
edition = {Second},
keywords = {machine learning},
publisher = {Springer},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
url = {http://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr{\_}1{\_}14?ie=UTF8{\&}qid=1429565346{\&}sr=8-14{\&}keywords=machine+learning},
year = {2011}
}

@article{herbster1998tracking,
  title={Tracking the best expert},
  author={Herbster, Mark and Warmuth, Manfred K},
  journal={Machine Learning},
  volume={32},
  number={2},
  pages={151--178},
  year={1998},
  publisher={Springer}
}

@article{Hyndman2008,
abstract = {Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.},
author = {Hyndman, Rob J. and Khandakar, Yeasmin},
doi = {10.18637/jss.v027.i03},
isbn = {1548-7660},
issn = {10411135},
journal = {Journal Of Statistical Software},
keywords = {arima models,automatic forecasting,exponential smoothing,prediction inter,r,state space models,time series,vals},
number = {3},
pages = {C3--C3},
pmid = {258205200001},
title = {{Automatic time series forecasting: The forecast package for R}},
url = {http://www.robjhyndman.com/papers/forecastpackage.pdf},
volume = {27},
year = {2008}
}

@inproceedings{jahrer2010Netflix,
  title={Combining predictions for accurate recommender systems},
  author={Jahrer, Michael and T{\"o}scher, Andreas and Legenstein, Robert},
  booktitle={Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={693--702},
  year={2010},
  organization={ACM}
}

@article{Krishnamurti2000,
abstract = {Abstract In this paper the performance of a multimodel ensemble forecast analysis that shows superior forecast skills is illustrated and compared to all individual models used. The model comparisons include global weather, hurricane track and intensity forecasts, and seasonal climate simulations. The performance improvements are completely attributed to the collective information of all models used in the statistical algorithm. The proposed concept is first illustrated for a low-order spectral model from which the multimodels and a “nature run” were constructed. Two hundred time units are divided into a training period (70 time units) and a forecast period (130 time units). The multimodel forecasts and the observed fields (the nature run) during the training period are subjected to a simple linear multiple regression to derive the statistical weights for the member models. The multimodel forecasts, generated for the next 130 forecast units, outperform all the individual models. This procedure was deployed...},
author = {Krishnamurti, T. N. and Kishtawal, C. M. and Zhang, Zhan and LaRow, Timothy and Bachiochi, David and Williford, Eric and Gadgil, Sulochana and Surendran, Sajani and Krishnamurti, T. N. and Kishtawal, C. M. and Zhang, Zhan and LaRow, Timothy and Bachiochi, David and Williford, Eric and Gadgil, Sulochana and Surendran, Sajani},
doi = {10.1175/1520-0442(2000)013<4196:MEFFWA>2.0.CO;2},
issn = {0894-8755},
journal = {Journal of Climate},
month = {dec},
number = {23},
pages = {4196--4216},
title = {{Multimodel Ensemble Forecasts for Weather and Seasonal Climate}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0442{\%}282000{\%}29013{\%}3C4196{\%}3AMEFFWA{\%}3E2.0.CO{\%}3B2},
volume = {13},
year = {2000}
}

@Manual{Leydold2015,
    title = {rstream: Streams of Random Numbers},
    author = {Josef Leydold},
    year = {2015},
    note = {R package version 1.3.4},
    url = {https://CRAN.R-project.org/package=rstream},
}

@incollection{Lin2004,
address = {Berlin, Heidelberg},
author = {Lin, Xiaodong and Zhu, Yu},
booktitle = {Classification, Clustering, and Data Mining Applications},
doi = {10.1007/978-3-642-17103-1_25},
file = {::},
pages = {259--268},
publisher = {Springer Berlin Heidelberg},
title = {{Degenerate Expectation-Maximization Algorithm for Local Dimension Reduction}},
url = {http://link.springer.com/10.1007/978-3-642-17103-1{\_}25},
year = {2004}
}

@article{Lindstrom2015,
author = {Lindstr{\"{o}}m, Tom and Tildesley, Michael and Webb, Colleen},
file = {:Users/ngr/Documents/Mendeley Desktop/Lindstr{\"{o}}m, Tildesley, Webb/Lindstr{\"{o}}m, Tildesley, Webb - 2015.pdf:pdf},
journal = {PLoS computational biology},
number = {4},
publisher = {Public Library of Science},
title = {{A Bayesian Ensemble Approach for Epidemiological Projections}},
url = {file:///pmc/articles/PMC4415763/?report=abstract papers2://publication/doi/10.1371/journal.pcbi.1004187},
volume = {11},
year = {2015}
}

@article{Madigan1998,
abstract = {Standard statistical practice ignores model un-certainty. Data analysts typically select a model from some class of models and then proceed as if the model generated the data. This approach ignores a component of uncertainty, leading to over-conndent inferences. Bayesian model aver-aging BMA provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and pro-vide pointers to a number of applications. In these applications, BMA provides improved out-of-sample predictive performance. We provide a catalogue of currently available BMA software.},
author = {Madigan, David and Raftery, A and Volinsky and Hoeting, J},
file = {:Users/ngr/Documents/Mendeley Desktop/Madigan et al/Madigan et al. - 1998.pdf:pdf},
journal = {Proceedings of the AAAI Workshop on Integrating Multiple Learned Models, Portland, OR.},
number = {9814},
title = {{Bayesian Model Averaging}},
year = {1998}
}

@techreport{PandemicPredictionandForecastingScienceandTechnologyWorkingGroupoftheNationalScienceandTechnologyCouncil2016,
author = {{Pandemic Prediction and Forecasting Science and Technology Working Group of the National Science and Technology Council}},
file = {:Users/ngr/Documents/Mendeley Desktop/Pandemic Prediction and Forecasting Science and Technology Working Group of the National Science and Technology Council/Pandemic Prediction and Forecasting Science and Technology Working Group of the National Science and Technology Council - 2016.pdf:pdf},
title = {{Towards Epidemic Prediction: Federal Efforts and Opportunities in Outbreak Modeling}},
url = {https://www.whitehouse.gov/sites/default/files/microsites/ostp/NSTC/towards{\_}epidemic{\_}prediction-federal{\_}efforts{\_}and{\_}opportunities.pdf},
year = {2016}
}

@article{polikar2006ensemble,
  title={Ensemble based systems in decision making},
  author={Polikar, Robi},
  journal={IEEE Circuits and systems magazine},
  volume={6},
  number={3},
  pages={21--45},
  year={2006},
  publisher={IEEE}
}

@Manual{Rcore2016,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2016},
    url = {https://www.R-project.org/},
}

@article{Raftery2005,
abstract = {Abstract Ensembles used for probabilistic weather forecasting often exhibit a spread-error correlation, but they tend to be underdispersive. This paper proposes a statistical method for postprocessing ensembles based on Bayesian model averaging (BMA), which is a standard method for combining predictive distributions from different sources. The BMA predictive probability density function (PDF) of any quantity of interest is a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights are equal to posterior probabilities of the models generating the forecasts and reflect the models' relative contributions to predictive skill over the training period. The BMA weights can be used to assess the usefulness of ensemble members, and this can be used as a basis for selecting ensemble members; this can be useful given the cost of running large ensembles. The BMA PDF can be represented as an unweighted ensemble of any desired size, by simulating from the BMA predictive distributi...},
author = {Raftery, Adrian E and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
file = {:Users/ngr/Documents/Mendeley Desktop/Raftery et al/Raftery et al. - 2005.pdf:pdf},
journal = {Monthly Weather Review},
number = {5},
pages = {1155--1174},
title = {{Using Bayesian Model Averaging to Calibrate Forecast Ensembles}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/MWR2906.1 papers2://publication/doi/10.1175/MWR2906.1},
volume = {133},
year = {2005}
}

@article{rigollet2007linearconvexaggregationdensity,
  title={Linear and convex aggregation of density estimators},
  author={Rigollet, Ph and Tsybakov, Alexander B},
  journal={Mathematical Methods of Statistics},
  volume={16},
  number={3},
  pages={260--280},
  year={2007},
  publisher={Springer}
}

@inproceedings{rosset2002boosting,
  title={Boosting density estimation},
  author={Rosset, Saharon and Segal, Eran},
  booktitle={NIPS},
  pages={641--648},
  year={2002}
}

@article{Seni2010,
abstract = {Abstract Ensemble methods have been called the most influential development in Data Mining and Machine Learning in the past decade. They combine multiple models into one usually more accurate than the best of its components. Ensembles can provide a critical boost to industrial challenges -- from investment timing to drug discovery, and fraud detection to recommendation systems -- where predictive accuracy is more vital than model interpretability. Ensembles are useful with all modeling algorithms, but this book focuses on decision trees to explain them most clearly. After describing trees and their strengths and weaknesses, the authors provide an overview of regularization -- today understood to be a key reason for the superior performance of modern ensembling algorithms. The book continues with a clear description of two recent developments: Importance Sampling (IS) and Rule Ensembles (RE). IS reveals classic ensemble methods -- bagging, random forests, and boosting -- to be special cases of a single alg...},
author = {Seni, Giovanni and Elder, John F},
file = {:Users/ngr/Documents/Mendeley Desktop/Seni, Elder/Seni, Elder - 2010.pdf:pdf},
journal = {Synthesis Lectures on Data Mining and Knowledge Discovery},
number = {1},
pages = {1--126},
publisher = {Morgan {\&} Claypool Publishers },
title = {{Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions }},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00240ED1V01Y200912DMK002 papers2://publication/doi/10.2200/S00240ED1V01Y200912DMK002},
volume = {2},
year = {2010}
}

@article{Shaman2012,
abstract = {Influenza recurs seasonally in temperate regions of the world; however, our ability to predict the timing, duration, and magnitude of local seasonal outbreaks of influenza remains limited. Here we develop a framework for initializing real-time forecasts of seasonal influenza outbreaks, using a data assimilation technique commonly applied in numerical weather prediction. The availability of real-time, web-based estimates of local influenza infection rates makes this type of quantitative forecasting possible. Retrospective ensemble forecasts are generated on a weekly basis following assimilation of these web-based estimates for the 2003-2008 influenza seasons in New York City. The findings indicate that real-time skillful predictions of peak timing can be made more than 7 wk in advance of the actual peak. In addition, confidence in those predictions can be inferred from the spread of the forecast ensemble. This work represents an initial step in the development of a statistically rigorous system for real-time forecast of seasonal influenza.},
address = {Department of Environmental Health Sciences, Mailman School of Public Health, Columbia University, New York, NY 10032, USA. jls106@columbia.edu},
author = {Shaman, Jeffrey and Karspeck, Alicia},
file = {:Users/ngr/Documents/Mendeley Desktop/Shaman, Karspeck/Shaman, Karspeck - 2012.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {50},
pages = {20425--20430},
title = {{Forecasting seasonal outbreaks of influenza.}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=23184969{\&}retmode=ref{\&}cmd=prlinks papers2://publication/doi/10.1073/pnas.1208772109},
volume = {109},
year = {2012}
}

@article{Sill2009,
abstract = {Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset.},
archivePrefix = {arXiv},
arxivId = {0911.0460},
author = {Sill, Joseph and Takacs, Gabor and Mackey, Lester and Lin, David},
eprint = {0911.0460},
file = {:Users/ngr/Documents/Mendeley Desktop/Sill et al/Sill et al. - 2009.pdf:pdf},
month = {nov},
title = {{Feature-Weighted Linear Stacking}},
url = {http://arxiv.org/abs/0911.0460},
year = {2009}
}

@book{silverman1986density,
  title={Density estimation for statistics and data analysis},
  author={Silverman, Bernard W},
  volume={26},
  year={1986},
  publisher={CRC press}
}

@article{smyth1999stackingDensityEstimators,
  title={Linearly combining density estimators via stacking},
  author={Smyth, Padhraic and Wolpert, David},
  journal={Machine Learning},
  volume={36},
  number={1-2},
  pages={59--83},
  year={1999},
  publisher={Springer}
}

@article{Tebaldi2007,
author = {Tebaldi, Claudia and Knutti, Reto},
journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
number = {1857},
title = {{The use of the multi-model ensemble in probabilistic climate projections}},
volume = {365},
year = {2007}
}

@article{Wolpert1992,
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
author = {Wolpert, David H},
journal = {Neural Networks},
number = {2},
pages = {241--259},
title = {{Stacked generalization}},
url = {http://pubget.com/site/paper/2a043750-5667-424d-94f1-2e4e39fccbcc?institution= papers2://publication/doi/10.1016/S0893-6080(05)80023-1},
volume = {5},
year = {1992}
}

@article{Yamana2016,
abstract = {In recent years, a number of systems capable of predicting future infectious disease incidence have been developed. As more of these systems are operationalized, it is important that the forecasts generated by these different approaches be formally reconciled so that individual forecast error and bias are reduced. Here we present a first example of such multi-system, or superensemble, forecast. We develop three distinct systems for predicting dengue, which are applied retrospectively to forecast outbreak characteristics in San Juan, Puerto Rico. We then use Bayesian averaging methods to combine the predictions from these systems and create superensemble forecasts. We demonstrate that on average, the superensemble approach produces more accurate forecasts than those made from any of the individual forecasting systems.},
author = {Yamana, Teresa K and Kandula, Sasikiran and Shaman, Jeffrey},
file = {:Users/ngr/Documents/Mendeley Desktop/Yamana, Kandula, Shaman/Yamana, Kandula, Shaman - 2016.pdf:pdf},
journal = {Journal of The Royal Society Interface},
number = {123},
pages = {20160410},
publisher = {The Royal Society},
title = {{Superensemble forecasts of dengue outbreaks}},
url = {http://rsif.royalsocietypublishing.org/content/13/123/20160410.abstract papers2://publication/doi/10.1098/rsif.2016.0410},
volume = {13},
year = {2016}
}

@article{yamanishi2007dynamicmodelselection,
  title={Dynamic model selection with its applications to novelty detection},
  author={Yamanishi, Kenji and Maruyama, Yuko},
  journal={IEEE Transactions on Information Theory},
  volume={53},
  number={6},
  pages={2180--2189},
  year={2007},
  publisher={IEEE}
}
